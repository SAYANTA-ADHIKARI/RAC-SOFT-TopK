model_name: bert-base-uncased
language: en
task: contrastive
train_file: ./data/preprocessed/English_train.json
eval_file: ./data/preprocessed/English_dev.json
num_neg_samples: 3
max_seq_length: 128
margin: 0.5
output_dir: ./output/contrastive/
epochs: 1
batch_size: 1
warmup_steps: 100
weight_decay: 0.01
logging_steps: 10
eval_steps: 100
save_steps: 100
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
learning_rate: 0.00005
lr_schedular_type: cosine